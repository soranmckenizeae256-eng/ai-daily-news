import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path

current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

try:
    import requests
    import feedparser
    from bs4 import BeautifulSoup
    from openai import OpenAI
    import httpx
except ImportError as e:
    logging.error(f"å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    logging.info("è¯·è¿è¡Œ: pip install -r requirements.txt")
    sys.exit(1)

from config import (
    FEISHU_WEBHOOK_URL,
    NEWS_CATEGORIES,
    CATEGORY_KEYWORDS,
    DATETIME_FORMAT,
    MAX_NEWS_PER_CATEGORY,
    TOTAL_NEWS_COUNT,
    RSS_SOURCES,
    HTTP_SOURCES,
    AI_PROVIDER,
    OPENAI_API_KEY,
    DEEPSEEK_API_KEY,
    DEEPSEEK_API_BASE
)

logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        self.articles = []
<<<<<<< HEAD
        self.client = None
        self.ai_provider = AI_PROVIDER.lower()
        
        logger.info(f"AIæä¾›å•†é…ç½®: {self.ai_provider}")
        logger.info(f"DEEPSEEK_API_KEYæ˜¯å¦å­˜åœ¨: {'æ˜¯' if DEEPSEEK_API_KEY else 'å¦'}")
        logger.info(f"OPENAI_API_KEYæ˜¯å¦å­˜åœ¨: {'æ˜¯' if OPENAI_API_KEY else 'å¦'}")
        
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        if self.ai_provider == 'deepseek':
            if DEEPSEEK_API_KEY:
                logger.info("æ­£åœ¨åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯...")
                self.client = OpenAI(
                    api_key=DEEPSEEK_API_KEY,
                    base_url=DEEPSEEK_API_BASE
                )
                logger.info("DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.error("DeepSeek APIå¯†é’¥ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯")
        elif self.ai_provider == 'openai':
            if OPENAI_API_KEY:
                logger.info("æ­£åœ¨åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯...")
                self.client = OpenAI(
                    api_key=OPENAI_API_KEY
                )
                logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.error("OpenAI APIå¯†é’¥ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå§‹åŒ–å®¢æˆ·ç«¯")
        
        if not self.client:
            logger.warning("æœªé…ç½®æœ‰æ•ˆçš„AI APIå¯†é’¥ï¼Œå°†ä½¿ç”¨åŸå§‹æ ‡é¢˜")
=======
        self.client = OpenAI(
            api_key=os.environ.get('OPENAI_API_KEY', ''),
<<<<<<< HEAD
=======
            http_client=httpx(proxies=os.environ.get('HTTP_PROXY', None))
>>>>>>> 8af3ff9 (feat: åˆå§‹åŒ–AIæ–°é—»æ—¥æŠ¥é¡¹ç›®)
        )
>>>>>>> a60790c (feat: æ·»åŠ è¯¦ç»†è°ƒè¯•æ—¥å¿—å’Œé…ç½®æ–‡ä»¶ä¿®å¤)
        
    def parse_rss_feed(self, feed_url, source_name):
        """è§£æRSSè®¢é˜…æº"""
        try:
            logger.info(f"æ­£åœ¨è§£æRSSæº: {source_name}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            response = httpx.get(feed_url, headers=headers, timeout=30.0)
            response.raise_for_status()
            
            feed = feedparser.parse(response.text)
            
            for entry in feed.entries[:10]:
                article = {
                    'title': entry.get('title', '').strip(),
                    'link': entry.get('link', '').strip(),
                    'published': entry.get('published', '').strip() or entry.get('updated', '').strip(),
                    'summary': entry.get('summary', '').strip(),
                    'source': source_name,
                    'language': 'en'
                }
                if article['title'] and article['link']:
                    self.articles.append(article)
                    
            logger.info(f"  âœ“ ä» {source_name} è·å– {min(10, len(feed.entries))} æ¡æ–°é—»")
            
        except Exception as e:
            logger.error(f"  âœ— è§£æ {source_name} å¤±è´¥: {e}")
            
    def fetch_hacker_news(self, source_config):
        """è·å–Hacker News"""
        try:
            logger.info(f"æ­£åœ¨è·å–: {source_config['name']}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            response = httpx.get(source_config['url'], headers=headers, timeout=30.0)
            response.raise_for_status()
            
            data = response.json()
            hn_items = data.get('data', {}).get('children', [])[:30]
            
            for item in hn_items:
                story = item.get('data', {})
                if story.get('score', 0) >= 50:
                    title = story.get('title', '')
                    url = f"https://news.ycombinator.com/item?id={story.get('id')}"
                    
                    article = {
                        'title': title,
                        'link': url,
                        'published': datetime.now().isoformat(),
                        'summary': '',
                        'source': source_config['name'],
                        'language': 'en',
                        'score': story.get('score', 0)
                    }
                    self.articles.append(article)
                    
            logger.info(f"  âœ“ ä» {source_config['name']} è·å–çƒ­é—¨æ–°é—»")
            
        except Exception as e:
            logger.error(f"  âœ— è·å– {source_config['name']} å¤±è´¥: {e}")
            
    def fetch_reddit(self, source_config):
        """è·å–Redditæ•°æ®"""
        try:
            logger.info(f"æ­£åœ¨è·å–: {source_config['name']}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            response = httpx.get(source_config['url'], headers=headers, timeout=30.0)
            response.raise_for_status()
            
            data = response.json()
            posts = data.get('data', {}).get('children', [])[:20]
            
            for post in posts:
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                self_url = f"https://www.reddit.com{post_data.get('permalink', '')}"
                
                article = {
                    'title': title,
                    'link': self_url,
                    'published': datetime.fromtimestamp(post_data.get('created_utc', 0)).isoformat(),
                    'summary': post_data.get('selftext', '')[:500],
                    'source': source_config['name'],
                    'language': 'en',
                    'score': post_data.get('score', 0)
                }
                self.articles.append(article)
                
            logger.info(f"  âœ“ ä» {source_config['name']} è·å– {len(posts)} æ¡æ–°é—»")
            
        except Exception as e:
            logger.error(f"  âœ— è·å– {source_config['name']} å¤±è´¥: {e}")
            
    def fetch_weibo(self, source_config):
        """è·å–å¾®åšæ•°æ®"""
        try:
            logger.info(f"æ­£åœ¨è·å–: {source_config['name']}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'cookie': os.environ.get('WEIBO_COOKIE', '')
            }
            response = httpx.get(source_config['url'], headers=headers, timeout=30.0)
            response.raise_for_status()
            
            data = response.json()
            if data.get('ok') == 1:
                list_data = data.get('list', [])
                for item in list_data[:10]:
                    text = item.get('text_raw', '') or item.get('text', '')
                    article = {
                        'title': text[:100] + '...' if len(text) > 100 else text,
                        'link': f"https://weibo.com/0/statuses/{item.get('mid', '')}",
                        'published': item.get('created_at', ''),
                        'summary': text,
                        'source': source_config['name'],
                        'language': 'zh',
                        'attitudes_count': item.get('attitudes_count', 0)
                    }
                    if article['title']:
                        self.articles.append(article)
                        
            logger.info(f"  âœ“ ä» {source_config['name']} è·å–å¾®åšåŠ¨æ€")
            
        except Exception as e:
            logger.error(f"  âœ— è·å– {source_config['name']} å¤±è´¥: {e}")
            
    def collect_all_news(self):
        """æ”¶é›†æ‰€æœ‰æ–°é—»"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ”¶é›†AIæ–°é—»...")
        logger.info("=" * 60)
        
        logger.info("\nğŸ“¡ è§£æRSSè®¢é˜…æº...")
        for category, feeds in RSS_SOURCES.items():
            for feed_url in feeds:
                source_name = feed_url.split('//')[1].split('/')[0]
                self.parse_rss_feed(feed_url, source_name)
                
        logger.info("\nğŸŒ æŠ“å–ç½‘é¡µæ–°é—»æº...")
        for category, sources in HTTP_SOURCES.items():
            for source in sources:
                if 'Hacker' in source['name']:
                    self.fetch_hacker_news(source)
                elif 'Reddit' in source['name']:
                    self.fetch_reddit(source)
                elif 'å¾®åš' in source['name']:
                    self.fetch_weibo(source)
                    
        logger.info(f"\nâœ… å…±æ”¶é›†åˆ° {len(self.articles)} æ¡æ–°é—»")
        return self.articles
        
    def categorize_article(self, article):
        """ä¸ºæ–‡ç« åˆ†ç±»"""
        title_lower = article['title'].lower()
        summary_lower = (article.get('summary', '') or '').lower()
        text = title_lower + ' ' + summary_lower
        
        for category, keywords in CATEGORY_KEYWORDS.items():
            if category == 'ğŸ“Š å…¶ä»–è¦é—»':
                continue
            for keyword in keywords:
                if keyword.lower() in text:
                    return category
                    
        return 'ğŸ“Š å…¶ä»–è¦é—»'
        
    def translate_to_chinese(self, text):
        """å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒå£è¯­åŒ–"""
        if not self.client:
            logger.warning("AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ç¿»è¯‘")
            return text
            
        try:
            logger.info(f"å¼€å§‹ç¿»è¯‘æ–‡æœ¬: {text[:50]}...")
            prompt = f"""
è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼Œè¦æ±‚ï¼š
1. ä¿æŒå£è¯­åŒ–ï¼Œç¬¦åˆä¸­å›½äººçš„é˜…è¯»ä¹ æƒ¯
2. ä¸è¦ç›´è¯‘ï¼Œè¦æµç•…è‡ªç„¶
3. ä¿ç•™æ–°é—»çš„æ ¸å¿ƒä¿¡æ¯
4. ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Š

æ–‡æœ¬ï¼š{text}
"""
            
            # æ ¹æ®AIæä¾›å•†é€‰æ‹©åˆé€‚çš„æ¨¡å‹
            model = "deepseek-chat" if self.ai_provider == 'deepseek' else "gpt-3.5-turbo"
            logger.info(f"ä½¿ç”¨æ¨¡å‹: {model}")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.7
            )
            
            translation = response.choices[0].message.content.strip()
            logger.info(f"ç¿»è¯‘ç»“æœ: {translation}")
            return translation if translation else text
            
        except Exception as e:
            logger.error(f"ç¿»è¯‘å¤±è´¥: {e}")
            return text
            
    def summarize_with_ai(self, article):
        """ä½¿ç”¨AIæ€»ç»“æ–‡ç« ï¼Œç”Ÿæˆä¸­æ–‡å£è¯­åŒ–æ‘˜è¦"""
        title = article['title']
        summary = article.get('summary', '')[:500]
        
        logger.info(f"å¤„ç†æ–‡ç« : {title[:50]}...")
        is_chinese = any(ord(c) > 127 for c in title)
        logger.info(f"æ ‡é¢˜æ˜¯å¦ä¸ºä¸­æ–‡: {'æ˜¯' if is_chinese else 'å¦'}")
        
        # å…ˆå°†è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡
        if is_chinese:
            chinese_title = title
            logger.info(f"æ ‡é¢˜ä¸ºä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨: {chinese_title}")
        else:
            chinese_title = self.translate_to_chinese(title)
            article['translated_title'] = chinese_title
            logger.info(f"è‹±æ–‡æ ‡é¢˜ç¿»è¯‘ä¸º: {chinese_title}")
        
        if not self.client:
            logger.info("AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜ä½œä¸ºæ‘˜è¦")
            return chinese_title
            
        try:
            logger.info("å¼€å§‹ç”Ÿæˆæ‘˜è¦...")
            prompt = f"""
è¯·ç”¨ä¸€å¥å¤§ç™½è¯æ€»ç»“ä»¥ä¸‹AIæ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆ20-30å­—ä»¥å†…ï¼‰ï¼Œä½¿å…¶é€šä¿—æ˜“æ‡‚ï¼š

æ ‡é¢˜: {chinese_title}
æ‘˜è¦: {summary}

è¦æ±‚ï¼š
1. ç”¨ç®€å•çš„ä¸­æ–‡å£è¯­è§£é‡Šæ–°é—»å†…å®¹
2. ä¸è¦åŒ…å«å…¬å¸åç§°
3. ç›´æ¥è¾“å‡ºæ€»ç»“ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Š
"""
            
            # æ ¹æ®AIæä¾›å•†é€‰æ‹©åˆé€‚çš„æ¨¡å‹
            model = "deepseek-chat" if self.ai_provider == 'deepseek' else "gpt-3.5-turbo"
            logger.info(f"ä½¿ç”¨æ¨¡å‹: {model}")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50,
                temperature=0.7
            )
            
            summary_text = response.choices[0].message.content.strip()
            logger.info(f"ç”Ÿæˆçš„æ‘˜è¦: {summary_text}")
            return summary_text if summary_text else chinese_title
            
        except Exception as e:
            logger.error(f"AIæ€»ç»“å¤±è´¥: {e}")
            return chinese_title
            
    def filter_and_categorize(self, articles):
        """è¿‡æ»¤å’Œåˆ†ç±»æ–‡ç« """
        unique_articles = {}
        for article in articles:
            link = article['link']
            if link not in unique_articles:
                unique_articles[link] = article
                
        unique_articles = list(unique_articles.values())
        
        categorized = {cat: [] for cat in NEWS_CATEGORIES.keys()}
        
        for article in unique_articles:
            category = self.categorize_article(article)
            article['category'] = category
            categorized[category].append(article)
            
        for category in categorized:
            categorized[category].sort(
                key=lambda x: x.get('score', 0) if 'score' in x else 0, 
                reverse=True
            )
            
        final_selection = []
        for category, articles in categorized.items():
            final_selection.extend(articles[:MAX_NEWS_PER_CATEGORY])
            
        if len(final_selection) > TOTAL_NEWS_COUNT:
            sorted_articles = sorted(
                final_selection, 
                key=lambda x: x.get('score', 0) if 'score' in x else 0, 
                reverse=True
            )
            final_selection = sorted_articles[:TOTAL_NEWS_COUNT]
            
        return final_selection
        
    def process_articles(self, articles):
        """å¤„ç†å’Œæ€»ç»“æ–‡ç« """
        logger.info("\nğŸ”„ æ­£åœ¨å¤„ç†å’Œæ€»ç»“æ–°é—»...")
        
        selected_articles = self.filter_and_categorize(articles)
        
        processed_articles = []
        for i, article in enumerate(selected_articles, 1):
            article['index'] = i
            article['summary_ai'] = self.summarize_with_ai(article)
            processed_articles.append(article)
            
        logger.info(f"  âœ“ å¤„ç†å®Œæˆ {len(processed_articles)} æ¡æ–°é—»")
        return processed_articles
        
    def generate_daily_report(self, articles, date_str=None):
        """ç”Ÿæˆæ¯æ—¥æ—¥æŠ¥"""
        if not date_str:
            date_str = datetime.now().strftime(DATETIME_FORMAT)
            
        report_lines = []
        report_lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        report_lines.append(f"ğŸ“… {date_str} AIæ–°é—»æ—¥æŠ¥")
        report_lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        report_lines.append("")
        
        categories_order = ['ğŸš€ äº§å“å‘å¸ƒ', 'ğŸ’° æŠ•èèµ„', 'ğŸ”¬ æŠ€æœ¯çªç ´', 'ğŸ¯ è¡Œä¸šè§‚ç‚¹', 'ğŸ“Š å…¶ä»–è¦é—»']
        
        for category in categories_order:
            category_articles = [a for a in articles if a.get('category') == category]
            if category_articles:
                report_lines.append(f"{category}")
                for article in category_articles:
                    # ä½¿ç”¨ç¿»è¯‘åçš„æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹æ ‡é¢˜
                    display_title = article.get('translated_title', article['title'])
                    report_lines.append(f"{article['index']}. **{display_title}**")
                    report_lines.append(f"   ğŸ“ {article['summary_ai']}")
                    report_lines.append(f"   ğŸ”— {article['link']}")
                    report_lines.append("")
                    
        report_lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        report_lines.append(f"â° æ¯å¤©æ—©ä¸Š8:00è‡ªåŠ¨æ¨é€ | å…±{len(articles)}æ¡è¦é—»")
        
        return '\n'.join(report_lines)
